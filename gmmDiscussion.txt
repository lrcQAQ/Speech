------------------ Experiment 1: M ------------------
Fix maxIter = 20, S = 32, change M to be [1, 2, 3, 5, 8, 10]

M	Accuracy
1	1.0
2	0.9375
3	0.96875
5	1.0
8	1.0
10	1.0

Discussion:
For smaller M (number of Gaussian Mixtures), the result can be less accurate, but the amount penalized is small (<0.1), the result can still be considered good. 

------------------ Experiment 2: maxIter ------------------
Fix M = 8, S = 32, change maxIter to be [0, 2, 5, 8, 10]

maxIter	Accuracy
1	0.96875
2	1.0
5	1.0
8	1.0
10	1.0

Discussion:
Smaller number of maxIter limit number of steps we run EM algorithms. Worse performance is observed when maxIter = 1. However, it is reasonable to conclude EM algorithm converges really quick on our dataset, since accuracy stays 100% start from maxIter = 2.

------------------ Experiment 3: S ------------------
Fix M = 8, maxIter = 20, change S to be [5, 10, 15, 20, 25, 32]
S	Accuracy
5	0.15625
10	0.3125
15	0.46875
20	0.625
25	0.78125
32	1.0

Discussion:
Less number of speakers lead to worse accuracy. As number of speakers gets smaller, the accuracy gradually gets lower as well. Moreover, the accuracy is equal to S/32.

------------------ Experiment 4: epsilon ------------------
Fix M = 8, maxIter = 20, change epsilon to [0, 10, 100, 1000, 10000]
epsilon 	Accuracy
0	1.0
10	1.0
100	1.0
1000	1.0
10000	0.96875

Discussion:
Smaller epsilon lead to better accuracy. This is because smaller epsilon will let EM algorithm to runs more, until better converged. Since epsilon controls the error allowed for EM algorithm.

------------------ Questions ------------------
1. How might you improve the classification accuracy of the Gaussian mixtures, without adding more
training data?
We can decrease epsilon, increase number of speakers, increase M and increase maxIters. And we can make better initialization on theta.

2. When would your classifier decide that a given test utterance comes from none of the trained speaker
models, and how would your classifier come to this decision?

Our model will always calculate all likelihoods and find the smallest one, it can't distinguish the case where test utterance comes from non of the trained speaker models. To overcome this, we can set a threshold, when the largest computed likelihood is below such threshold, we classify it to none.

3. Can you think of some alternative methods for doing speaker identification that don't use Gaussian
mixtures?

It's a multiclass classification problem, hence, K-mean can be used, where each cluster of k-mean represents all characteristic of each speakers. 
We can also use RNN, which have good performance managing audio data.

